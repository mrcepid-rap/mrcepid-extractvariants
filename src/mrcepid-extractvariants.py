#!/usr/bin/env python
# mrcepid-runnassociationtesting 0.0.1
# Generated by dx-app-wizard.
#
# Author: Eugene Gardner (eugene.gardner at mrc.epid.cam.ac.uk)
#
# DNAnexus Python Bindings (dxpy) documentation:
#   http://autodoc.dnanexus.com/bindings/python/current/

import dxpy
import csv
import subprocess
import tarfile
import pandas as pd
import pandas.core.series
import statsmodels.api as sm
import numpy as np
import os
import gzip
from os.path import exists
from pandas.core.series import Series


# This is to generate a global CHROMOSOMES variable for parallelisation
CHROMOSOMES = list(range(1,23)) # Is 0 based on the right coordinate...? (So does 1..22)
CHROMOSOMES.extend(['X'])
CHROMOSOMES = list(map(str, CHROMOSOMES))


# This function runs a command on an instance, either with or without calling the docker instance we downloaded
# By default, commands are not run via Docker, but can be changed by setting is_docker = True
# Also, by default, standard out is not saved, but can be modified with the 'stdout_file' parameter.
def run_cmd(cmd: str, is_docker: bool = False, stdout_file: str = None) -> None:

    # -v here mounts a local directory on an instance (in this case the home dir) to a directory internal to the
    # Docker instance named /test/. This allows us to run commands on files stored on the AWS instance within Docker.
    # This looks slightly different from other versions of this command I have written as I needed to write a custom
    # R script to run STAAR. That means we have multiple mounts here to enable this code to find the script.
    if is_docker:
        cmd = "docker run " \
              "-v /home/dnanexus:/test " \
              "egardner413/mrcepid-associationtesting " + cmd

    # Standard python calling external commands protocol
    proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = proc.communicate()
    if stdout_file is not None:
        with open(stdout_file, 'w') as stdout_writer:
            stdout_writer.write(stdout.decode('utf-8'))
        stdout_writer.close()

    # If the command doesn't work, print the error stream and close the AWS instance out with 'dxpy.AppError'
    if proc.returncode != 0:
        print("The following cmd failed:")
        print(cmd)
        print("STDERROR follows\n")
        print(stderr.decode('utf-8'))
        raise dxpy.AppError("Failed to run properly...")


# This function is slightly different that in other applets I have designed. This function handles ALL inputs rather
# than just external dependencies
def ingest_data(association_tarball: str, phenofile: str, covarfile: str, inclusion_list: str, exclusion_list: str,
                base_covariates: dict, fam_file: dict, transcript_index: dict, bgen_index: dict) -> dict:

    # Get covariate/phenotype data:
    dxpy.download_dxfile(dxpy.DXFile(base_covariates).get_id(), 'base_covariates.covariates')
    dxpy.download_dxfile(dxpy.DXFile(phenofile).get_id(), 'model_phenotypes.pheno')
    dxpy.download_dxfile(dxpy.DXFile(fam_file).get_id(), 'UKBB_450K_Autosomes_QCd.fam')
    dxpy.download_dxfile(dxpy.DXFile(transcript_index).get_id(), 'transcripts.tsv.gz')

    # Now on to the more difficult stuff...
    # Need to grab the tarball file for associations...
    # This was generated by the applet mrcepid-collapsevariants
    # Ingest the list file into this AWS instance
    tarball_prefixes = []

    tarball = dxpy.DXFile(association_tarball)
    tarball_name = tarball.describe()['name']
    dxpy.download_dxfile(tarball, tarball_name)

    # Need to get the prefix on the tarball to access resources within:
    # All files within SHOULD have the same prefix as this file
    tarball_prefix = tarball_name.rstrip('.tar.gz')
    cmd = "tar -zxf " + tarball_name # unzip the tarball:
    run_cmd(cmd)

    # Get vep indicies to annotate variants from:
    # Ingest the INDEX of bgen files:
    bgen_dict = {}
    bgen_index = dxpy.DXFile(bgen_index)
    dxpy.download_dxfile(bgen_index.get_id(), "bgen_locs.tsv")
    # and load it into a dict:
    os.mkdir("filtered_bgen/") # For downloading later...
    bgen_index_csv = csv.DictReader(open("bgen_locs.tsv", "r"), delimiter="\t")
    for line in bgen_index_csv:
        bgen_dict[line['chrom']] = {'vep': line['vep_dxid']}

    # Check if additional covariates were provided:
    additional_covariates_found = False # default is we DID NOT find a file
    if covarfile is not None:
        covarfile = dxpy.DXFile(covarfile)
        dxpy.download_dxfile(covarfile, 'additional_covariates.covariates')
        additional_covariates_found = True

    # Get inclusion/exclusion sample lists
    inclusion_found = False # default is we DID NOT find a file
    if inclusion_list is not None:
        inclusion_list = dxpy.DXFile(inclusion_list)
        dxpy.download_dxfile(inclusion_list, 'INCLUSION.lst')
        inclusion_found = True
    exclusion_found = False # default is we DID NOT find a file
    if exclusion_list is not None:
        exclusion_list = dxpy.DXFile(exclusion_list)
        dxpy.download_dxfile(exclusion_list, 'EXCLUSION.lst')
        exclusion_found = True

    return {'tarball_prefix': tarball_prefix,
            'inclusion_found': inclusion_found,
            'exclusion_found': exclusion_found,
            'additional_covariates_found': additional_covariates_found,
            'bgen_dict': bgen_dict}


# Three steps here:
# 1. Get individuals we plan to include
# 2. Exclude individuals not wanted in the analysis
# 3. Get individuals that are POSSIBLE to include (they actually have WES) and only keep 'include' samples
def select_individuals(inclusion_found: bool, exclusion_found: bool) -> set:

    # Get a list of individuals that we ARE going to use
    include_samples = set()
    if inclusion_found is True:
        inclusion_file = open('INCLUSION.lst', 'r')
        for indv in inclusion_file:
            indv = indv.rstrip()
            include_samples.add(indv)

    # Get a list of individuals that we ARE NOT going to use
    exclude_samples = set()
    if exclusion_found is True:
        exclude_file = open('EXCLUSION.lst', 'r')
        for indv in exclude_file:
            indv = indv.rstrip()
            exclude_samples.add(indv)

    # Get individuals with genetic data
    # Remember! the genetic data has already been filtered to individuals with WES data.
    genetics_fam_file = open('UKBB_450K_Autosomes_QCd.fam', 'r')
    genetics_samples = set()
    for line in genetics_fam_file:
        line = line.rstrip()
        fields = line.split()
        eid = fields[0]
        if inclusion_found == False and exclusion_found == False:
            genetics_samples.add(eid)
        elif inclusion_found == False and exclusion_found == True:
            if eid not in exclude_samples:
                genetics_samples.add(eid)
        elif inclusion_found == True and exclusion_found == False:
            if eid in include_samples:
                genetics_samples.add(eid)
        else:
            if eid in include_samples and eid not in exclude_samples:
                genetics_samples.add(eid)

    print("Total samples after inclusion/exclusion lists applied: %i" % len(genetics_samples))
    return genetics_samples


# This is a helper function for 'create_covariate_file()' that processes the phenotype file
def process_phenotype() -> tuple:
    # Need to go through phenofile first and injest into a dictionary and get the name of the phenofield:
    dialect = csv.Sniffer().sniff(open('model_phenotypes.pheno', 'r').readline(), delimiters=[' ','\t'])
    pheno_reader = csv.DictReader(open('model_phenotypes.pheno', 'r'), delimiter=dialect.delimiter, skipinitialspace=True)
    field_names = pheno_reader.fieldnames
    if len(field_names) != 3:
        raise RuntimeError("Pheno file has more than three columns!")
    elif "FID" not in field_names and "IID" not in field_names:
        raise RuntimeError("Pheno file has column names other than FID/IID/Phenotype!")

    for field in field_names:
        if field != "FID" and field != "IID":
            pheno_name = field

    phenotypes = {}
    for indv in pheno_reader:
        # Will spit out an error if a given sample does not have data
        if indv[pheno_name] is None:
            raise dxpy.AppError("Phenotype file has blank lines!")
        # Exclude individuals that have missing data (NA/NAN)
        elif indv[pheno_name].lower() != "na" and indv[pheno_name].lower() != "nan" and indv[pheno_name].lower() != "":
            phenotypes[indv['FID']] = indv[pheno_name]

    return phenotypes, pheno_name


# This is a helper function for 'create_covariate_file()' that processes requested additional phenotypes
def process_additional_covariates(additional_covariates_found: bool, categorical_covariates: str, quantitative_covariates: str) -> tuple:

    if additional_covariates_found:
        dialect = csv.Sniffer().sniff(open('additional_covariates.covariates', 'r').readline(), delimiters=[' ','\t'])
        additional_covar_reader = csv.DictReader(open('additional_covariates.covariates', 'r'), delimiter=dialect.delimiter, skipinitialspace=True)
        field_names = list.copy(additional_covar_reader.fieldnames)

        # make sure the sample ID field is here and remove it from 'field_names' to help with iteration
        if 'FID' not in field_names and 'IID' not in field_names:
            raise dxpy.AppError("FID & IID column not found in provided covariates file!")
        else:
            field_names.remove('FID')
            field_names.remove('IID')

        # Now process & check the categorical/quantitative covariates lists and match it to field_names:
        found_categorical_covariates = []
        if categorical_covariates is not None:
            categorical_covariates = categorical_covariates.split(',')
            for covar in categorical_covariates:
                print(covar)
                if covar in field_names:
                    found_categorical_covariates.append(covar)
                else:
                    print("Provided categorical covariate %s not found in additional covariates file..." % covar)

        found_quantitative_covariates = []
        if quantitative_covariates is not None:
            quantitative_covariates = quantitative_covariates.split(',')
            for covar in quantitative_covariates:
                print(covar)
                if covar in field_names:
                    found_quantitative_covariates.append(covar)
                else:
                    print("Provided quantitative covariate %s not found in additional covariates file..." % covar)

        # Throw an error if user provided covariates but none were found
        if (len(found_categorical_covariates) + len(found_quantitative_covariates)) == 0:
            raise dxpy.AppError('Additional covariate file provided but no additional covariates found based on covariate names provided...')

        add_covars = {}
        for sample in additional_covar_reader:
            # First check no NAs/Blanks exist
            all_covars_found = True
            sample_dict = {}
            for field_name in (found_quantitative_covariates + found_categorical_covariates):
                if sample[field_name].lower() == "na" or sample[field_name].lower() == "nan" or sample[field_name].lower() == "":
                    all_covars_found = False
                else:
                    sample_dict[field_name] = sample[field_name]

            if all_covars_found == True:
                add_covars[sample['IID']] = sample_dict

        return add_covars, found_quantitative_covariates, found_categorical_covariates
    else:
        return {}, [], []


# Do covariate processing and sample inclusion/exclusion
def create_covariate_file(sex: int, genetics_samples: set, additional_covariates_found: bool, categorical_covariates: str,
                          quantitative_covariates: str) -> dict:

    # Process the phenotype:
    phenotypes, pheno_name = process_phenotype()

    # Process additional covariates (check if requested in the function):
    add_covars, found_quantitative_covariates, found_categorical_covariates = process_additional_covariates(additional_covariates_found,
                                                                                                            categorical_covariates,
                                                                                                            quantitative_covariates)
    # Read the base covariates into this code that we want to analyse:
    # Formatting is weird to fit with other printing below...
    print("Default covariates included in model                        :")
    print("    Quantitative                                            : age, age^2, PC1..PC10")
    if (sex == 2):
        print("    Categorical                                             : sex, WES_batch")
    else:
        print("    Categorical                                             : WES_batch")
    if additional_covariates_found:
        print("Number of individuals with non-null additional covariates   : %i" % len(add_covars))
        print("Additional covariates included in model                     :")
        if len(found_quantitative_covariates) > 0:
            print("    Quantitative                                            : " + ', '.join(found_quantitative_covariates))
        else:
            print("    Quantitative                                            : NULL")
        if len(found_quantitative_covariates) > 0:
            print("    Categorical                                             : " + ', '.join(found_categorical_covariates))
        else:
            print("    Categorical                                             : NULL")
    else:
        print("No additional covariates provided/found beyond defaults...")

    base_covar_reader = csv.DictReader(open('base_covariates.covariates', 'r'), delimiter="\t")
    indv_written = 0 # Just to count the number of samples we will analyse
    formatted_combo_file = open('phenotypes_covariates.formatted.txt', 'w') # SAIGE needs a combo file

    write_fields = ["FID", "IID"]
    write_fields = write_fields + ["PC%s" % (x) for x in range(1,41)]
    write_fields = write_fields + ["age", "age_squared", "sex", "wes_batch"]
    write_fields = write_fields + [pheno_name]
    # This doesn't matter to python if we didn't find additional covariates. A list of len() == 0 does not lengthen
    # the target list (e.g. 'write_fields')
    write_fields = write_fields + found_quantitative_covariates + found_categorical_covariates

    combo_writer = csv.DictWriter(formatted_combo_file,
                                  fieldnames = write_fields,
                                  quoting = csv.QUOTE_NONE,
                                  delimiter = " ",
                                  extrasaction='ignore')
    combo_writer.writeheader()

    # Need a list of included individuals ONLY:
    include_samples = open('SAMPLES_Include.txt', 'w')
    num_all_samples = 0
    na_pheno_samples = 0 # for checking number of individuals missing phenotype information
    for indv in base_covar_reader:
        if indv['22001-0.0'] != "": # need to exclude blank row individuals, eid is normally the only thing that shows up, so filter on sex
            indv_writer = {'FID': indv['eid'],
                           'IID': indv['eid']}
            for PC in range(1,41):
                old_PC = "22009-0.%s" % (PC)
                new_pc = "PC%s" % (PC)
                indv_writer[new_pc] = indv[old_PC]
            indv_writer['age'] = int(indv['21003-0.0'])
            indv_writer['age_squared'] = indv_writer['age']**2
            indv_writer['sex'] = int(indv['22001-0.0'])
            indv_writer['wes_batch'] = indv['wes.batch']

            # Check if we found additional covariates and make sure this sample has non-null values
            write_sample = False
            if len(add_covars) > 0:
                if indv['eid'] in add_covars:
                    write_sample = True
                    for covariate in add_covars[indv['eid']]:
                        indv_writer[covariate] = add_covars[indv['eid']][covariate]
            else:
                write_sample = True

            # exclude based on sex-specific analysis if required:
            if indv['eid'] in genetics_samples:
                num_all_samples += 1
                if indv['eid'] in phenotypes and write_sample:
                    indv_writer[pheno_name] = phenotypes[indv['eid']]
                    if sex == 2:
                        indv_written += 1
                        combo_writer.writerow(indv_writer)
                        include_samples.write(indv['eid'] + "\n")
                    elif sex == indv_writer['sex']:
                        indv_written += 1
                        combo_writer.writerow(indv_writer)
                        include_samples.write(indv['eid'] + "\n")
                else:
                    na_pheno_samples += 1

    formatted_combo_file.close()
    include_samples.close()

    # Print to ensure that total number of individuals is consistent between genetic and covariate/phenotype data
    print("Samples with covariates after include/exclude lists applied : %i" % num_all_samples)
    print("Number of individuals with NaN/NA phenotype information     : %i" % na_pheno_samples)
    print("Number of individuals written to covariate/pheno file       : %i" % indv_written)

    # Return the phenotype name and additional covariates (if any) so we can use it later
    return {'pheno_name': pheno_name,
            'quant_covars': found_quantitative_covariates,
            'cat_covars': found_categorical_covariates}


def get_gene_id(gene_id) -> pandas.core.series.Series:

    transcripts_table = pd.read_csv(gzip.open('transcripts.tsv.gz', 'rt'), sep = "\t")
    transcripts_table = transcripts_table.rename(columns={'#chrom':'chrom'})
    transcripts_table = transcripts_table.set_index('ENST')

    if 'ENST' in gene_id:
        print("gene_id – " + gene_id + " – looks like an ENST value... validating...")
        try:
            gene_info = transcripts_table.loc[gene_id]
            print("Found one matching ENST (%s - %s)... proceeding..." % (gene_id, gene_info['coord']))
        except KeyError:
            print("Did not find a transcript with ENST value %s... terminating..." % gene_id)
    else:
        print("gene_id – " + gene_id + " – does not look like an ENST value, searching for symbol instead...")
        found_rows = transcripts_table[transcripts_table['SYMBOL'] == gene_id]
        if len(found_rows) == 1:
            found_enst = found_rows.index[0]
            gene_info = transcripts_table.loc[found_enst]
            print("Found one matching ENST (%s - %s) for SYMBOL %s... proceeding..." % (found_enst, gene_info['coord'], gene_id))
        elif len(found_rows) > 1:
            gene_info = None
            print("Found %i ENST IDs (%s) for SYMBOL %s... Please re-run using exact ENST to ensure consistent results..." % (len(found_rows), ','.join(found_rows.index.to_list()), gene_id))
            raise Exception("Multiple ENST IDs")
        else:
            gene_info = None
            print("Did not find an associated ENST ID for SYMBOL %s... Please re-run after checking SYMBOL/ENST used..." % (gene_id))

    return gene_info


# Setup linear models:
# I really don't like how I supply the association_pack here, but I think easiest way to do it...
def linear_model_null(association_pack: dict) -> dict:

    # load covariates and phenotypes
    pheno_covars = pd.read_csv("phenotypes_covariates.formatted.txt",
                               sep=" ",
                               index_col="FID",
                               dtype={'IID':str})
    pheno_covars.index = pheno_covars.index.astype(str)

    # Decide what model family to use:
    if association_pack['is_binary']:
        family = sm.families.Binomial()
    else:
        family = sm.families.Gaussian()

    # And finally define the formula to be used by all models:
    # Make sure to define additional covariates as requested by the user...
    form_null = association_pack['pheno_name'] + ' ~ sex + age + age_squared + C(wes_batch) + PC1 + PC2 + PC3 + PC4 + PC5 + PC6 + PC7 + PC8 + PC9 + PC10'
    if len(association_pack['quant_covars']) > 0:
        for covar in association_pack['quant_covars']:
            form_null += ' + ' + covar
    if len(association_pack['cat_covars']) > 0:
        for covar in association_pack['cat_covars']:
            form_null += ' + C(' + covar + ')'

    form_full = form_null + ' + has_var'

    print('Using the following formula for GLMs: ')
    print(form_full)

    # Build the null model and extract residuals:
    print('fitting NULL model')
    sm_results_null = sm.GLM.from_formula(form_null, data=pheno_covars, family=sm.families.Gaussian()).fit()
    null_table = sm_results_null.resid_response.to_frame()
    null_table = null_table.rename(columns={0:'resid'})

    # Start building a set of models we want to test.
    linear_model_pack = {'phenotypes': pheno_covars,
                         'pheno_name': association_pack['pheno_name'],
                         'model_family': family,
                         'model_formula': form_full,
                         'n_model': len(pheno_covars),
                         'null_model': null_table,
                         'genotypes': {}} # 'genotypes' is an empty dict that we will fill with individual mask tables

    return linear_model_pack


# load genes/genetic data we want to test/use:
# For each tarball prefix, we want to make ONE dict for efficient querying of variants
def load_tarball_linear_model(tarball_prefix: str, gene_info: pandas.core.series.Series) -> tuple:

    chromosome = gene_info['chrom']
    # This handles the genes that we need to test:
    if exists(tarball_prefix + "." + chromosome + ".BOLT.bgen"):
        # This handles the actual genetic data:
        # load genetic data
        # first convert into format we can use:
        cmd = "plink2 --threads 4 --bgen /test/" + tarball_prefix + "." + chromosome + ".BOLT.bgen 'ref-last' " \
                      "--sample /test/" + tarball_prefix + "." + chromosome + ".BOLT.sample " \
                      "--export bcf --out /test/lm." + tarball_prefix + "." + chromosome + " " + \
                      "--keep-fam /test/SAMPLES_Include.txt"
        run_cmd(cmd, True)

        # This just makes a sparse matrix with columns: sample_id, gene name, genotype
        cmd = "bcftools query -i \"GT='alt'\" -f \"[%SAMPLE\\t%ID\\t%GT\\n]\" " \
                      "/test/lm." + tarball_prefix + "." + chromosome + ".bcf > " \
                      "lm." + tarball_prefix + "." + chromosome + ".tsv"
        run_cmd(cmd, True)

        geno_table = pd.read_csv("lm." + tarball_prefix + "." + chromosome + ".tsv",
                                 sep = "\t",
                                 names = ['eid', 'gene', 'gt'])
        # bgen stores samples in eid_eid format, so get just the first eid
        geno_table[['eid','eid2']] = geno_table['eid'].str.split('_', 1, expand=True)
        geno_table = geno_table.drop('eid2', axis=1)

        # Get all possible genes found and aggregate across gene IDs to get a list of individuals with a given gene
        geno_table = geno_table.groupby('gene').agg({'eid': Series.to_list})
    else:
        print("Chromosome %s does not exist for tarball %s... Are you sure you used the correct gene_id? Terminating..." % (chromosome, tarball_prefix))
        raise Exception("Chromosome not found")

    # And concatenate the final data_frame together:
    genetic_data = geno_table.to_dict()
    genetic_data = genetic_data['eid'][gene_info.name] # no idea why there is a top level 'eid' key

    return genetic_data


def annotate_variants(tarball_prefix: str, bgen_dict: dict, gene_info: pandas.core.series.Series):

    chromosome = gene_info['chrom']
    vep = dxpy.DXFile(bgen_dict[chromosome]['vep'])
    dxpy.download_dxfile(vep.get_id(), chromosome + ".filtered.vep.tsv.gz")

    variant_index = pd.read_csv(gzip.open(chromosome + ".filtered.vep.tsv.gz", 'rt'), sep = "\t")

    # Need to get the variants from the SAIGE groupfile:
    with open(tarball_prefix + "." + chromosome + ".SAIGE.groupFile.txt") as saige_group_file:
        var_ids = []
        var_file = open('variants.txt', 'w')
        for line in saige_group_file:
            data = line.rstrip().split("\t")
            if data[0] == gene_info.name:
                for i in range(1,len(data)):
                    currID = data[i].replace('_',':').replace('/',':')
                    var_file.write(currID + "\n")
                    var_ids.append(currID)
                break
        var_file.close()

    relevant_vars = variant_index[variant_index['varID'].isin(var_ids)]

    # And filter the relevant SAIGE file to just the individuals we want so we can get actual MAC
    cmd = "bcftools view --threads 4 -S /test/SAMPLES_Include.txt -Ob -o /test/" + tarball_prefix + "." + chromosome + ".saige_input.bcf /test/" + tarball_prefix + "." + chromosome + ".SAIGE.bcf"
    run_cmd(cmd, True)
    cmd = "bcftools view --threads 4 -i \'ID=@/test/variants.txt\' -Ob -o /test/" + tarball_prefix + "." + chromosome + ".variant_filtered.bcf /test/" + tarball_prefix + "." + chromosome + ".saige_input.bcf"
    run_cmd(cmd, True)
    cmd = "bcftools +fill-tags --threads 4 -Ob -o /test/" + tarball_prefix + "." + chromosome + ".final.bcf /test/" + tarball_prefix + "." + chromosome + ".variant_filtered.bcf"
    run_cmd(cmd, True)

    # Now get actual annotations back in:
    cmd = "bcftools query -f \'%ID\\t%MAF\\t%AC\\t%AC_Het\\t%AC_Hom\\n\' -o /test/annotated_vars.txt /test/" + tarball_prefix + "." + chromosome + ".final.bcf"
    run_cmd(cmd, True)
    # And get a list of individuals with a variant:
    cmd = "bcftools query -i \"GT=\'alt\'\" -f \'[%CHROM\\t%POS\\t%ID\\t%REF\\t%ALT\\t%SAMPLE\\t%GT\n]\' -o /test/carriers.txt /test/" + tarball_prefix + "." + chromosome + ".final.bcf"
    run_cmd(cmd, True)

    geno_table = pd.read_csv("annotated_vars.txt",
                             sep = "\t",
                             names = ['varID','MAF_tested','AC_tested','AC_tested_Het','AC_tested_Hom'])
    geno_table = pd.merge(relevant_vars, geno_table, on='varID', how="left")

    carriers_table = pd.read_csv("carriers.txt",
                                 sep = "\t",
                                 names = ['CHROM','POS','varID','REF','ALT','IID','GT'])

    geno_table.to_csv(path_or_buf='variant_table.tsv', index = False, sep="\t", na_rep='NA')
    carriers_table.to_csv(path_or_buf='carriers_formated.tsv', index = False, sep="\t", na_rep='NA')


# Run rare variant association testing using GLMs
def linear_model_genes(linear_model_pack: dict, indv_w_var: list) -> dict:

    # Now successively iterate through each gene and run our model:
    # I think this is straight-forward
    n_car = len(indv_w_var)
    if n_car <= 2:
        gene_dict = {'n_car': n_car,
                     'n_model': linear_model_pack['n_model'],
                     'pheno_name': linear_model_pack['pheno_name'],
                     'p_val': 'NA',
                     'effect': 'NA',
                     'std_err': 'NA'}
    else:
        internal_frame = linear_model_pack['phenotypes']
        internal_frame['has_var'] = np.where(internal_frame.index.isin(indv_w_var), 1, 0)
        sm_results_full = sm.GLM.from_formula(linear_model_pack['model_formula'],
                                              data=internal_frame,
                                              family=linear_model_pack['model_family']).fit()
        gene_dict = {'n_car': n_car,
                     'n_model': sm_results_full.nobs,
                     'pheno_name': linear_model_pack['pheno_name'],
                     'p_val': sm_results_full.pvalues['has_var'],
                     'effect': sm_results_full.params['has_var'],
                     'std_err': sm_results_full.bse['has_var']}

    return gene_dict


@dxpy.entry_point('main')
def main(gene_id, association_tarball, is_binary, sex, exclusion_list, inclusion_list, phenofile, covarfile,
         categorical_covariates, quantitative_covariates, output_prefix, base_covariates, fam_file, transcript_index, bgen_index):

    # First get number of cores available to the instance:
    threads = os.cpu_count()
    print('Number of threads available: %i' % threads)

    # Bring our docker image into our environment so that we can run commands we need:
    cmd = "docker pull egardner413/mrcepid-associationtesting:latest"
    run_cmd(cmd)

    # Grab the data necessary to run this:
    injested_data_info = ingest_data(association_tarball, phenofile, covarfile, inclusion_list, exclusion_list, base_covariates, fam_file, transcript_index, bgen_index)

    # This does sample and covariate processing for all pipelines regardless of what we need to run
    # Also returns the phenotype name we are going to test
    genetics_samples = select_individuals(injested_data_info['inclusion_found'],
                                          injested_data_info['exclusion_found'])
    association_pack = create_covariate_file(sex,
                                             genetics_samples,
                                             injested_data_info['additional_covariates_found'],
                                             categorical_covariates,
                                             quantitative_covariates)
    gene_info = get_gene_id(gene_id)

    print("Phenotype: " + association_pack['pheno_name'])

    # Attach additional required information to the 'association_pack' variable to enable easy function running:
    association_pack['tarball_prefix'] = injested_data_info['tarball_prefix']
    association_pack['is_binary'] = is_binary
    association_pack['sex'] = sex
    association_pack['output_prefix'] = output_prefix

    # Run models that were selected – first checking if 'run_all' is enabled:
    output_files = [] # Create a list of outputs to drop into the tarball later

    # Then run selected models
    # Set up a thread pool executor to parallelise by chromosome where required:
    # Need to pare down tested samples to those in the covariate file using bcftools:
    # Running each chromosome on a separate thread to speed things up

    print("Running Linear Model")
    # First, do setup for the linear models.
    # This will load all variants, genes, and phenotypes into memory to allow for parallelization
    # This function returns a dictionary with the following keys:
    # 'genes': The list of genes to iterate over
    # 'phenotypes': Phenotypes/covariates for every individual
    # 'genotypes': Sparse matrix of genotypes in pandas data.frame format
    # 'model_formula': Formatted formula for all linear models
    # 'n_model': Number of individuals with values in the pheno/covariate file (do this here to save compute in threads)
    # 'genotypes': A dictionary storing "genotypes" for all masks provided to this tool (empty here, we fill below)
    print("Running Null Linear Model")
    linear_model_pack = linear_model_null(association_pack)

    # And load the tarballs INTO the genotypes dictionary
    print("Loading genotypes/variants")
    indv_with_var = load_tarball_linear_model(association_pack['tarball_prefix'], gene_info = gene_info)
    annotate_variants(association_pack['tarball_prefix'],injested_data_info['bgen_dict'],gene_info)
    print("Finished loading genotypes")

    print("Submitting model")
    # Next we are going to iterate through every model / gene (in linear_model_pack['genes']) pair and run a GLM
    returns = linear_model_genes(linear_model_pack = linear_model_pack,indv_w_var= indv_with_var)

    returns['gene'] = gene_info.name
    returns['mask'] = association_pack['tarball_prefix']
    with open('association_stats.tsv', 'w') as linear_out:
        linear_out.write('ENST\tmask\tn_car\tn_model\tpheno_name\tp_val_full\teffect\tstd_err\n')
        linear_out.write('{gene}\t{mask}\t{n_car}\t{n_model}\t{pheno_name}\t{p_val}\t{effect}\t{std_err}\n'.format(**returns))
        linear_out.close()
        print('{0:20}: {val}'.format("Gene ID", val = returns['gene']))
        print('{0:20}: {val}'.format("Mask", val = returns['mask']))
        print('{0:20}: {val}'.format("Phenotype", val = returns['pheno_name']))
        print('{0:20}: {val}'.format("N", val = returns['n_model']))
        print('{0:20}: {val}'.format("N Carriers", val = returns['n_car']))
        print('{0:20}: {val:0.3e}'.format("Linear Model p", val = returns['p_val']))
        print('{0:20}: {val:0.2f}'.format("Effect/OR", val = returns['effect']))
        print('{0:20}: {val:0.2f}'.format("std. err.", val = returns['std_err']))

    # Create tar of all possible output files
    if output_prefix is None:
        output_prefix = gene_id # Have to do this to make sure we can name files below
        output_tarball = "assoc_stats.tar.gz"
    else:
        output_tarball = output_prefix + ".assoc_stats.tar.gz"

    # Files so far:
    # phenotypes/covariates – phenotypes_covariates.formatted.txt
    # list of carriers with alleles – carriers_formated.tsv
    # annotated variants – variant_table.tsv
    # lm results – association_stats.tsv
    tar = tarfile.open(output_tarball, "w:gz")
    os.rename('phenotypes_covariates.formatted.txt', output_prefix + '.phenotypes_covariates.formatted.tsv')
    tar.add(output_prefix + '.phenotypes_covariates.formatted.tsv')

    os.rename('carriers_formated.tsv', output_prefix + '.carriers_formated.tsv')
    tar.add(output_prefix + '.carriers_formated.tsv')

    os.rename('variant_table.tsv', output_prefix + '.variant_table.tsv')
    tar.add(output_prefix + '.variant_table.tsv')

    os.rename('association_stats.tsv', output_prefix + '.association_stats.tsv')
    tar.add(output_prefix + '.association_stats.tsv')
    tar.close()

    ## Have to do 'upload_local_file' to make sure the new file is registered with dna nexus
    output = {"output_tarball": dxpy.dxlink(dxpy.upload_local_file(output_tarball))}

    return output


dxpy.run()
